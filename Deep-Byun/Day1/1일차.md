# DeepByun 1일차  
  
* Linear Regression : 연속적인(Linear) 값을 예측하는 것
  
* Classfication : 연속적인 값을 Discrete하게 바꿔서 예측하는 것   
  
* loss와 cost의 차이점  
많이들 같이 쓰기도 하지만, 실제로는 다른 값이다.  
loss : 각 데이터에 대해서의 모델의 예측 값과 실제 레이블의 차이    
cost : 실제값과 모델의 예측 값의 평균적인 차이   
  
  
  
## Linear Regression  
  
  
연속적인(Linear) 값을 예측하는 것   
  
  
### Cost function  
  
Θ = Θ - (σJ(Θ) / σΘ)  : Θ를 위의 식으로 업데이트한다.     
하지만 위의 식 그대로 반영하게 되면 오히려 발산할 수 있기 때문에 작은 값을 곱해서 업데이트 해준다.   
이때의 작은 값 상수 α가 Learning rate (학습율) 이라고 불린다.  
:: Θ = Θ - α(σJ(Θ) / σΘ)      
경식 형의 Tip !!  
Iteration을 체크할 때 홀수로 해라 => 한번 발산해서 튀었다가 다시 돌아올 확률이 높다   
  
  
  
## Backpropagation  
   
Chain Rule을 멋있게 표현했을 뿐이라고도 말한다.  
결국은 각 변수들이 얼마나 영향을 미치는지를 계산해서  
:: Θ = Θ - α(σJ(Θ) / σΘ) 식을 이용해서 계산해준다 
  
## Normalization  
  
Cost Function이 Scale에 의해 모양이 변할 수 있다.  
어떠한 범위를 가지고 있는지에 따라 학습이 어느 정도로 이루어 지는지가 결정될 수 있다.  
  

## Batch  
  
오차역전파를 하나하나 따라가서 생각해보면, 배치사이즈는 작을수록 좋다.  
왜냐면 역전파의 평균을 반영하기 때문에 배치사이즈가 크면 별로 좋지 않다.  
=> 보통 32보다 크게 하지 않는다.  
  
하지만 배치사이즈가 크면 좋은 점이 있다. => 노이즈에 둔감해진다.   
  
  





