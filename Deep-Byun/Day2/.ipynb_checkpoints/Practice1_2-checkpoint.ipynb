{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Practice.1-2\n",
    "## Forward and Backward Propagation with $y = \\theta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 Practice1-2에서는 datapoint마다의 loss로 parameter를 update하지 않고, datapoint 5개의 loss들을 이용하여 cost를 구하고, 이 cost부터 backpropagation으로 parameter를 update하는 과정을 연습한다.\n",
    "\n",
    "먼저 전 Practice의 Dataset Generation 부분과 Node Implmentation 부분을 가져오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = np.arange(1, 6)\n",
    "y_data = np.arange(1, 6)\n",
    "\n",
    "class MinusNode():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x - y\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dL):\n",
    "        return dL, -1*dL\n",
    "\n",
    "class MulNode():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x*y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return self.y*dL, self.x*dL\n",
    "\n",
    "class SquareNode():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x, self.z = x, x*x\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dL):\n",
    "        return 2*self.z*dL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 Practice1-1에서 구한 loss들은 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "$l^{(1)}, l^{(2)}, l^{(3)}, l^{(4)}, l^{(5)}$\n",
    "\n",
    "이때 Cost는 다음과 같이 정리된다.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} l^{(i)}$\n",
    "\n",
    "따라서 이번 Practice1-2에서는 $l^{(i)}$들을 구한 뒤, 위의 식을 이용하여 cost를 구하고\n",
    "\n",
    "cost node에서부터 backprogation을 진행한다.\n",
    "\n",
    "먼저 cost를 구하는 node를 만들어보자.\n",
    "\n",
    "이때 Vectorization을 이용하기 위하여 cost_node()는 $l^{(i)}$들을 ndarray로 받아 평균치를 구해 최종 cost를 구할 것이고, backward에서는 datapoint 개수와 일치하는 차원의 ndarray를 return한다.\n",
    "\n",
    "즉, forward()에서는 np.mean()을 이용하고\n",
    "\n",
    "backward()에서는 (N,) ndarray를 만들어 gradient를 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost node(forward):  4.0\n",
      "cost node(backward):  [0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "class CostNode():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "        self.length = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.length = len(x)\n",
    "        self.z = np.mean(x)\n",
    "        return self.z\n",
    "        \n",
    "    def backward(self):\n",
    "        return np.array([(1 / self.length)] * self.length)\n",
    "    \n",
    "c_node = CostNode()\n",
    "test_gradient = np.arange(2,7)\n",
    "print(\"cost node(forward): \", c_node.forward(test_gradient))\n",
    "print(\"cost node(backward): \", c_node.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "cost node(forward):  4.0\n",
    "\n",
    "cost node(backward):  [0.2 0.2 0.2 0.2 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 cost_node를 이용하여 forward propagation, backward propagation을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'z3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-db9fc5d40f97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m##### Your Code(Forward Propagation) #####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mgradient_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'z3' is not defined"
     ]
    }
   ],
   "source": [
    "theta = 0\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "z1_node = MulNode()\n",
    "z2_node = MinusNode()\n",
    "z3_node = SquareNode()\n",
    "c_node = CostNode()\n",
    "\n",
    "loss_list = []\n",
    "theta_list = []\n",
    "for _ in range(epochs):\n",
    "    gradients = []\n",
    "    thetas = []\n",
    "    losses = []\n",
    "    for x, y in zip(x_data, y_data): \n",
    "        y_hat = z1_node.forward(x, theta)\n",
    "        error = z2_node.forward(y, y_hat)\n",
    "        gradients.append(z3_node.forward(error))\n",
    "        \n",
    "    cost = c_node.forward(np.array(gradients))\n",
    "    loss_list.append(cost)\n",
    "    dz = c_node.backward()\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        model.forward()\n",
    "        thetas.append([dtheta])\n",
    "    theta = theta - lr*np.mean(theta_np)\n",
    "    theta_list.append(theta)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(loss_list)\n",
    "ax.set_title(\"loss\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(theta_list)\n",
    "ax.set_title(r\"$\\theta$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"./images/1_2_image1.png\" width = 400>\n",
    "<img src=\"./images/1_2_image2.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)Practice1-1에서부터 곡선으로 바뀐 이유를 분석하시오.**\n",
    "\n",
    "**Q2)LR이 각각 0.001, 0.003, 0.01, 0.03, 0.1, 0.3일때의 결과를 비교하시오. 그리고 최적의 LR을 제시한 뒤 그 근거를 설명하시오.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
